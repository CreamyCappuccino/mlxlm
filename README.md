# ğŸ§  MLX-LM CLI Tool

This repository provides a simple CLI interface for working with MLX-based language models on Apple Silicon.  
It aims to offer a friendly, scriptable, and consistent way to explore and manage MLX-LM models.

---

## ğŸ“¦ Features

- ğŸ“‹ **List models** - View all installed MLX-LM models with sizes and aliases
- â„¹ï¸ **Model info** - Inspect model configuration, architecture, and precision
- ğŸ’¬ **Interactive chat** - Conversational mode with full context awareness
- ğŸ­ **Multiple renderers** - Harmony, HuggingFace, or plain text formatting
- ğŸ’¾ **Conversation history** - Toggle between full context or Q&A mode
- ğŸ›‘ **Stop sequences** - Fine-grained output control
- ğŸ·ï¸ **Alias management** - Quick shortcuts for long model names

---

## ğŸ› ï¸ Installation

### Prerequisites

- ğŸ **Python 3.10+**
- ğŸ **MLX framework** (Apple Silicon required)
- ğŸ“š **mlx-lm library** (and huggingface-hub)

### Quick Start

1. ğŸ“¥ **Clone the repository:**

```bash
git clone https://github.com/username/mlxlm.git
cd mlxlm
```

2. ğŸ“¦ **(Optional) Create a virtual environment:**

```bash
conda create -n mlxlm_env python=3.10
conda activate mlxlm_env
```

3. â¬‡ï¸ **Install dependencies:**

```bash
pip install mlx-lm huggingface-hub
```

4. ğŸ”— **Run the install script:**

```bash
chmod +x install.sh
./install.sh
```

This creates a symlink in `/usr/local/bin/mlxlm` so you can use `mlxlm` from anywhere.

5. âœ… **Verify installation:**

```bash
mlxlm list
```

### Manual Setup (Alternative)

If you prefer not to use the install script:

```bash
chmod +x mlxlm.py
sudo ln -s $(pwd)/mlxlm.py /usr/local/bin/mlxlm
```

---

## ğŸš€ Usage

### ğŸ“‹ List all models:

```bash
mlxlm list
```

### â„¹ï¸ Show model info:

```bash
mlxlm show <model-name>
```

### ğŸ’¬ Launch model in interactive chat mode:

```bash
mlxlm run <model-name>
```

**Options:**
- ğŸ­ `--chat {auto|harmony|hf|plain}`: Chat rendering mode (default: auto)
  - `auto`: Tries Harmony â†’ HuggingFace template â†’ plain
  - `harmony`: Official Harmony format (requires openai-harmony)
  - `hf`: HuggingFace chat_template
  - `plain`: Simple user/assistant format
- ğŸ’¾ `--history {on|off}`: Enable/disable conversation history (default: on)
  - `on`: Full conversation context (better for ongoing chats)
  - `off`: Q&A mode (each query independent, lower memory usage)
- ğŸ¤ `--system "text"`: Custom system prompt
- ğŸ“ `--max-tokens N`: Maximum tokens to generate per turn (default: 2048)
- âš¡ `--stream-mode {all|final|off}`: Control streaming output (default: all)
  - `all`: Stream all tokens in real-time
  - `final`: Stream only Harmony final channel content
  - `off`: Wait for complete response before displaying
- ğŸ›‘ `--stop "seq"`: Add stop sequences (can be repeated)
- â±ï¸ `--time-limit N`: Hard time limit per turn in seconds (0=off)
- ğŸ§  `--reasoning {low|medium|high}`: Hint for reasoning verbosity

**Examples:**
```bash
# ğŸ’¬ Full conversation with context
mlxlm run gemma3:27b --history on

# â“ Q&A mode (independent questions)
mlxlm run gemma3:27b --history off

# ğŸ¤ Custom system prompt
mlxlm run gemma3:27b --system "You are a helpful coding assistant"

# ğŸ“„ Plain text mode with max tokens limit
mlxlm run gemma3:27b --chat plain --max-tokens 512
```

### ğŸ·ï¸ Manage model aliases:

#### ğŸ¯ Interactive menu mode (recommended):

```bash
mlxlm alias
```

This launches an interactive menu where you can:
1. **Browse installed models** with their current aliases
2. **Select a model** by entering its number
3. **Add/edit alias** by typing a new name, or **remove** by pressing Enter with no input
4. **Confirm changes** and return to the menu for more operations

**Example flow:**
```
ğŸ§  Installed models:

1. models--google--gemma-3-27b-it  [gemma3]
2. models--meta--llama3-8b  [No alias]
0. Exit

ğŸ’¡ Tip: You can type /exit at any time to cancel the operation.

Enter model number: 1
Enter new alias to add or change, or leave blank to remove:
(Current: 'gemma3')
> gpt
Assign alias 'gpt' to 'models--google--gemma-3-27b-it'? [(y)/n]: y
âœ… Alias 'gpt' changed successfully!

ğŸ§  Installed models:
...
```

#### ğŸ”§ CLI alias commands:

```bash
# â• Add alias for a model
mlxlm alias add <model-name> <alias>

# âœï¸ Edit existing alias
mlxlm alias edit <old-alias> <new-alias>

# âŒ Remove an alias
mlxlm alias remove <alias>
```

### ğŸ©º Diagnose environment:

```bash
mlxlm doctor
```

âœ… Checks MLX runtime, Harmony renderer, HuggingFace cache, and required dependencies.

---

## ğŸ“ Folder Structure

```
MLX-LM/
â”œâ”€â”€ mlxlm.py            # Main CLI entry point
â”œâ”€â”€ commands.py         # Core command implementations (list, show, pull, run, alias, doctor)
â”œâ”€â”€ core.py             # Utility functions (model loading, rendering, streaming, Harmony support)
â”œâ”€â”€ cli_flags.py        # CLI argument parser definitions
â”œâ”€â”€ .mlxlm_aliases.json # Model alias mappings (auto-generated)
â”œâ”€â”€ README.md           # This file
â””â”€â”€ .gitignore
```

**Main source files:**
- ğŸ¯ `mlxlm.py`: Entry point, command routing
- âš™ï¸ `commands.py`: All command handlers (list, show, pull, remove, run, alias, doctor)
- ğŸ”§ `core.py`: Core utilities (config loading, model type detection, prompt rendering, streaming helpers)
- ğŸ“‹ `cli_flags.py`: Argument parser setup
- ğŸ” `LICENSE`: MIT License
- ğŸ“„ `README.md`: This documentation

---

## ğŸ“ Notes

- **Ollama-inspired**: Built to bring Ollama-like simplicity to MLX-LM
- **Apple Silicon optimized**: Leverages MLX framework for native performance
- **Model agnostic**: Works with any MLX-compatible model
- **Extensible**: Alias system and multiple chat modes for flexibility

---

## ğŸ¤ Contributing

Feedback and contributions welcome! Found a bug or have an idea? Feel free to open an issue.

---

## ğŸ“„ License

MIT License - See LICENSE file for details
